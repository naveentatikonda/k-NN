From b6a7c0392d3ddc73a9deb5aa8dc18627a6aa60ee Mon Sep 17 00:00:00 2001
From: Naveen Tatikonda <navtat@amazon.com>
Date: Wed, 3 Sep 2025 03:18:07 -0500
Subject: [PATCH] Optimize Faiss SQfp16

Signed-off-by: Naveen Tatikonda <navtat@amazon.com>
---
 faiss/impl/ScalarQuantizer.cpp | 207 +++++++++++++++++++++++++--------
 faiss/impl/platform_macros.h   |   2 +-
 2 files changed, 161 insertions(+), 48 deletions(-)

diff --git a/faiss/impl/ScalarQuantizer.cpp b/faiss/impl/ScalarQuantizer.cpp
index af90b7e13..9efdd7e45 100644
--- a/faiss/impl/ScalarQuantizer.cpp
+++ b/faiss/impl/ScalarQuantizer.cpp
@@ -651,13 +651,32 @@ struct QuantizerFP16<8> : QuantizerFP16<1> {
 
     FAISS_ALWAYS_INLINE float32x4x2_t
     reconstruct_8_components(const uint8_t* code, int i) const {
-        uint16x4x2_t codei = vld1_u16_x2((const uint16_t*)(code + 2 * i));
+        // Aligned pointer for faster loads (assumes Faiss codebook is 16B aligned)
+        const __restrict uint16_t* ptr = reinterpret_cast<const uint16_t*>(code + 2 * i);
+
+        // Prefetch next block to hide memory latency
+        __builtin_prefetch(ptr + 16, 0, 1);
+
+        // Load 8 FP16 values into NEON registers
+        uint16x4x2_t codei = vld1_u16_x2(ptr);
+
+        // Convert FP16 â†’ FP32 in one step
+#if defined(__aarch64__)  // AArch64 supports vcvt_high
+        float16x8_t f16 = vcombine_f16(vreinterpret_f16_u16(codei.val[0]),
+                                       vreinterpret_f16_u16(codei.val[1]));
+        float32x4_t f32_low  = vcvt_f32_f16(vget_low_f16(f16));
+        float32x4_t f32_high = vcvt_high_f32_f16(f16);
+        return {f32_low, f32_high};
+#else
+        // Portable fallback (still fast, but 2 calls instead of 1 packed op)
         return {vcvt_f32_f16(vreinterpret_f16_u16(codei.val[0])),
                 vcvt_f32_f16(vreinterpret_f16_u16(codei.val[1]))};
+#endif
     }
 };
 #endif
 
+
 /*******************************************************************
  * BF16 quantizer
  *******************************************************************/
@@ -1273,21 +1292,23 @@ struct SimilarityL2<8> {
     float32x4x2_t accu8;
 
     FAISS_ALWAYS_INLINE void begin_8() {
-        accu8 = {vdupq_n_f32(0.0f), vdupq_n_f32(0.0f)};
+        accu8.val[0] = vdupq_n_f32(0.0f);
+        accu8.val[1] = vdupq_n_f32(0.0f);
         yi = y;
     }
 
     FAISS_ALWAYS_INLINE void add_8_components(float32x4x2_t x) {
+        __builtin_prefetch(yi + 16, 0, 1);  // Prefetch next block
+
         float32x4x2_t yiv = vld1q_f32_x2(yi);
         yi += 8;
 
         float32x4_t sub0 = vsubq_f32(yiv.val[0], x.val[0]);
         float32x4_t sub1 = vsubq_f32(yiv.val[1], x.val[1]);
 
-        float32x4_t accu8_0 = vfmaq_f32(accu8.val[0], sub0, sub0);
-        float32x4_t accu8_1 = vfmaq_f32(accu8.val[1], sub1, sub1);
+        accu8.val[0] = vfmaq_f32(accu8.val[0], sub0, sub0);
+        accu8.val[1] = vfmaq_f32(accu8.val[1], sub1, sub1);
 
-        accu8 = {accu8_0, accu8_1};
     }
 
     FAISS_ALWAYS_INLINE void add_8_components_2(
@@ -1296,19 +1317,38 @@ struct SimilarityL2<8> {
         float32x4_t sub0 = vsubq_f32(y.val[0], x.val[0]);
         float32x4_t sub1 = vsubq_f32(y.val[1], x.val[1]);
 
-        float32x4_t accu8_0 = vfmaq_f32(accu8.val[0], sub0, sub0);
-        float32x4_t accu8_1 = vfmaq_f32(accu8.val[1], sub1, sub1);
+        accu8.val[0] = vfmaq_f32(accu8.val[0], sub0, sub0);
+        accu8.val[1] = vfmaq_f32(accu8.val[1], sub1, sub1);
 
-        accu8 = {accu8_0, accu8_1};
     }
 
     FAISS_ALWAYS_INLINE float result_8() {
-        float32x4_t sum_0 = vpaddq_f32(accu8.val[0], accu8.val[0]);
-        float32x4_t sum_1 = vpaddq_f32(accu8.val[1], accu8.val[1]);
-
-        float32x4_t sum2_0 = vpaddq_f32(sum_0, sum_0);
-        float32x4_t sum2_1 = vpaddq_f32(sum_1, sum_1);
-        return vgetq_lane_f32(sum2_0, 0) + vgetq_lane_f32(sum2_1, 0);
+    #if defined(__aarch64__) && __ARM_ARCH >= 8 && defined(__ARM_FEATURE_DIRECTED_ROUNDING)
+        // Fastest: ARMv8.4+ horizontal reduction (one instruction per vector)
+        return vaddvq_f32(accu8.val[0]) + vaddvq_f32(accu8.val[1]);
+
+    #elif defined(__aarch64__)
+        // Second best: pairwise reduction entirely in NEON registers
+        float32x4_t s0 = vpaddq_f32(accu8.val[0], accu8.val[0]);
+        float32x4_t s1 = vpaddq_f32(accu8.val[1], accu8.val[1]);
+        float32x4_t s2 = vpaddq_f32(s0, s0);
+        float32x4_t s3 = vpaddq_f32(s1, s1);
+        return vgetq_lane_f32(s2, 0) + vgetq_lane_f32(s3, 0);
+
+    #else
+        // Portable fallback: store + SIMD reduction (compiler will auto-vectorize)
+        float tmp0[4], tmp1[4];
+        vst1q_f32(tmp0, accu8.val[0]);
+        vst1q_f32(tmp1, accu8.val[1]);
+
+        float sum0 = 0.0f, sum1 = 0.0f;
+        #pragma omp simd reduction(+:sum0, sum1)
+        for (int i = 0; i < 4; i++) {
+            sum0 += tmp0[i];
+            sum1 += tmp1[i];
+        }
+        return sum0 + sum1;
+    #endif
     }
 };
 #endif
@@ -1423,6 +1463,7 @@ struct SimilarityIP<8> {
 #endif
 
 #ifdef USE_NEON
+#include <omp.h>
 
 template <>
 struct SimilarityIP<8> {
@@ -1435,36 +1476,55 @@ struct SimilarityIP<8> {
     float32x4x2_t accu8;
 
     FAISS_ALWAYS_INLINE void begin_8() {
-        accu8 = {vdupq_n_f32(0.0f), vdupq_n_f32(0.0f)};
+        accu8.val[0] = vdupq_n_f32(0.0f);
+        accu8.val[1] = vdupq_n_f32(0.0f);
         yi = y;
     }
 
     FAISS_ALWAYS_INLINE void add_8_components(float32x4x2_t x) {
+        __builtin_prefetch(yi + 16, 0, 1);  // Prefetch next block
+
         float32x4x2_t yiv = vld1q_f32_x2(yi);
         yi += 8;
 
-        float32x4_t accu8_0 = vfmaq_f32(accu8.val[0], yiv.val[0], x.val[0]);
-        float32x4_t accu8_1 = vfmaq_f32(accu8.val[1], yiv.val[1], x.val[1]);
-        accu8 = {accu8_0, accu8_1};
+        accu8.val[0] = vfmaq_f32(accu8.val[0], yiv.val[0], x.val[0]);
+        accu8.val[1] = vfmaq_f32(accu8.val[1], yiv.val[1], x.val[1]);
     }
 
     FAISS_ALWAYS_INLINE void add_8_components_2(
             float32x4x2_t x1,
             float32x4x2_t x2) {
-        float32x4_t accu8_0 = vfmaq_f32(accu8.val[0], x1.val[0], x2.val[0]);
-        float32x4_t accu8_1 = vfmaq_f32(accu8.val[1], x1.val[1], x2.val[1]);
-        accu8 = {accu8_0, accu8_1};
+        accu8.val[0] = vfmaq_f32(accu8.val[0], x1.val[0], x2.val[0]);
+        accu8.val[1] = vfmaq_f32(accu8.val[1], x1.val[1], x2.val[1]);
     }
 
     FAISS_ALWAYS_INLINE float result_8() {
-        float32x4x2_t sum = {
-                vpaddq_f32(accu8.val[0], accu8.val[0]),
-                vpaddq_f32(accu8.val[1], accu8.val[1])};
-
-        float32x4x2_t sum2 = {
-                vpaddq_f32(sum.val[0], sum.val[0]),
-                vpaddq_f32(sum.val[1], sum.val[1])};
-        return vgetq_lane_f32(sum2.val[0], 0) + vgetq_lane_f32(sum2.val[1], 0);
+    #if defined(__aarch64__) && __ARM_ARCH >= 8 && defined(__ARM_FEATURE_DIRECTED_ROUNDING)
+        // Fastest: ARMv8.4+ horizontal reduction (one instruction per vector)
+        return vaddvq_f32(accu8.val[0]) + vaddvq_f32(accu8.val[1]);
+
+    #elif defined(__aarch64__)
+        // Second best: pairwise reduction entirely in NEON registers
+        float32x4_t s0 = vpaddq_f32(accu8.val[0], accu8.val[0]);
+        float32x4_t s1 = vpaddq_f32(accu8.val[1], accu8.val[1]);
+        float32x4_t s2 = vpaddq_f32(s0, s0);
+        float32x4_t s3 = vpaddq_f32(s1, s1);
+        return vgetq_lane_f32(s2, 0) + vgetq_lane_f32(s3, 0);
+
+    #else
+        // Portable fallback: store + SIMD reduction (compiler will auto-vectorize)
+        float tmp0[4], tmp1[4];
+        vst1q_f32(tmp0, accu8.val[0]);
+        vst1q_f32(tmp1, accu8.val[1]);
+
+        float sum0 = 0.0f, sum1 = 0.0f;
+        #pragma omp simd reduction(+:sum0, sum1)
+        for (int i = 0; i < 4; i++) {
+            sum0 += tmp0[i];
+            sum1 += tmp1[i];
+        }
+        return sum0 + sum1;
+    #endif
     }
 };
 #endif
@@ -1629,27 +1689,80 @@ struct DCTemplate<Quantizer, Similarity, 8> : SQDistanceComputer {
 
     DCTemplate(size_t d, const std::vector<float>& trained)
             : quant(d, trained) {}
-    float compute_distance(const float* x, const uint8_t* code) const {
-        Similarity sim(x);
-        sim.begin_8();
-        for (size_t i = 0; i < quant.d; i += 8) {
-            float32x4x2_t xi = quant.reconstruct_8_components(code, i);
-            sim.add_8_components(xi);
+    float compute_distance(const float* __restrict x, const uint8_t* __restrict code) const {
+            FAISS_PRAGMA_IMPRECISE_FUNCTION_BEGIN
+            Sim sim(x);
+            sim.begin_8();
+
+            const size_t d = quant.d;
+            size_t i = 0;
+
+            // Unroll loop 4x (process 32 components per iteration)
+            for (; i + 32 <= d; i += 32) {
+                // Prefetch next data for quantized codes to improve cache
+                __builtin_prefetch(code + i, 0, 1);
+
+                float32x4x2_t xi0 = quant.reconstruct_8_components(code, i);
+                float32x4x2_t xi1 = quant.reconstruct_8_components(code, i + 8);
+                float32x4x2_t xi2 = quant.reconstruct_8_components(code, i + 16);
+                float32x4x2_t xi3 = quant.reconstruct_8_components(code, i + 24);
+
+                sim.add_8_components(xi0);
+                sim.add_8_components(xi1);
+                sim.add_8_components(xi2);
+                sim.add_8_components(xi3);
+            }
+
+            // Handle remainder
+            for (; i < d; i += 8) {
+                float32x4x2_t xi = quant.reconstruct_8_components(code, i);
+                sim.add_8_components(xi);
+            }
+
+            return sim.result_8();
+            FAISS_PRAGMA_IMPRECISE_FUNCTION_END
         }
-        return sim.result_8();
-    }
 
-    float compute_code_distance(const uint8_t* code1, const uint8_t* code2)
-            const {
-        Similarity sim(nullptr);
-        sim.begin_8();
-        for (size_t i = 0; i < quant.d; i += 8) {
-            float32x4x2_t x1 = quant.reconstruct_8_components(code1, i);
-            float32x4x2_t x2 = quant.reconstruct_8_components(code2, i);
-            sim.add_8_components_2(x1, x2);
+    // Compute distance between two quantized codes
+        float compute_code_distance(const uint8_t* __restrict code1, const uint8_t* __restrict code2) const {
+            FAISS_PRAGMA_IMPRECISE_FUNCTION_BEGIN
+            Sim sim(nullptr);
+            sim.begin_8();
+
+            const size_t d = quant.d;
+            size_t i = 0;
+
+            // Unroll loop 4x (process 32 components per iteration)
+            for (; i + 32 <= d; i += 32) {
+                __builtin_prefetch(code1 + i, 0, 1);
+                __builtin_prefetch(code2 + i, 0, 1);
+
+                float32x4x2_t x1_0 = quant.reconstruct_8_components(code1, i);
+                float32x4x2_t x1_1 = quant.reconstruct_8_components(code1, i + 8);
+                float32x4x2_t x1_2 = quant.reconstruct_8_components(code1, i + 16);
+                float32x4x2_t x1_3 = quant.reconstruct_8_components(code1, i + 24);
+
+                float32x4x2_t x2_0 = quant.reconstruct_8_components(code2, i);
+                float32x4x2_t x2_1 = quant.reconstruct_8_components(code2, i + 8);
+                float32x4x2_t x2_2 = quant.reconstruct_8_components(code2, i + 16);
+                float32x4x2_t x2_3 = quant.reconstruct_8_components(code2, i + 24);
+
+                sim.add_8_components_2(x1_0, x2_0);
+                sim.add_8_components_2(x1_1, x2_1);
+                sim.add_8_components_2(x1_2, x2_2);
+                sim.add_8_components_2(x1_3, x2_3);
+            }
+
+            // Handle remainder
+            for (; i < d; i += 8) {
+                float32x4x2_t x1 = quant.reconstruct_8_components(code1, i);
+                float32x4x2_t x2 = quant.reconstruct_8_components(code2, i);
+                sim.add_8_components_2(x1, x2);
+            }
+
+            return sim.result_8();
+            FAISS_PRAGMA_IMPRECISE_FUNCTION_END
         }
-        return sim.result_8();
-    }
 
     void set_query(const float* x) final {
         q = x;
diff --git a/faiss/impl/platform_macros.h b/faiss/impl/platform_macros.h
index 81b15427f..7d5ce45b0 100644
--- a/faiss/impl/platform_macros.h
+++ b/faiss/impl/platform_macros.h
@@ -177,7 +177,7 @@ inline int __builtin_clzll(uint64_t x) {
 #define FAISS_PRAGMA_IMPRECISE_LOOP
 #define FAISS_PRAGMA_IMPRECISE_FUNCTION_BEGIN \
     _Pragma("GCC push_options") \
-    _Pragma("GCC optimize (\"unroll-loops,associative-math,no-signed-zeros\")")
+    _Pragma("GCC optimize (\"O3,unroll-loops,ftree-vectorize,associative-math,no-signed-zeros,fno-math-errno,fstrict-aliasing,floop-interchange,floop-strip-mine,fpeel-loops,falign-loops=16\")")
 #define FAISS_PRAGMA_IMPRECISE_FUNCTION_END \
     _Pragma("GCC pop_options")
 #else
-- 
2.49.0

